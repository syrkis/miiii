\documentclass[11pt]{article} % 12pt font, specifies the document class
% preamble.tex
\input{/Users/syrkis/code/press/templates/preamble.tex} % Include the common preamble

% multicol
\usepackage{multicol}
\setcounter{unbalance}{2}
\raggedcolumns
\interlinepenalty=100
\widowpenalty=10000
\clubpenalty=10000
\flushbottom

% col margin
\setlength{\columnsep}{0.4in}

% Document-specific title information % custom title
\title{Mechanistic Interpretability on Irreducible Integers}
\author{Noah Syrkis \\ University of Copenhagen \\ noah@syrkis.com}
\date{}

% Document body
\begin{document}
\maketitle % Creates the title
\begin{multicols}{2}
  \begin{abstract}
  We apply the mechanistic interpretability framework to a transformer model trained on a dataset of irreducible integers. We show that the model has learned to perform modular addition, and we reverse-engineer the model to understand how it does so.
  \end{abstract}

  \section{Introduction}\label{introduction}

  Reverse-engineerings deep neural networks (DNN) is a relatively new
  field, but has already shown success. For example, reverse engineers a
  transformer model to understand how it performs modular addition.
  attempts to automate the reverse-engineering process, and is somewhat
  successful. Reverse-engineerings deep neural networks (DNN) is a
  relatively new field, but has already shown success. For example,
  reverse engineers a transformer model to understand how it performs
  modular addition. attempts to automate the reverse-engineering
  process, and is somewhat successful. Reverse-engineerings deep neural
  networks (DNN) is a relatively new field, but has already shown
  success. For example, reverse engineers a transformer model to
  understand how it performs modular addition. attempts to automate the
  reverse-engineering process, and is somewhat successful.
  Reverse-engineerings deep neural networks (DNN) is a relatively new
  field, but has already shown success. For example, reverse engineers a
  transformer model to understand how it performs modular addition.
  attempts to automate the reverse-engineering process, and is somewhat
  successful. Reverse-engineerings deep neural networks (DNN) is a
  relatively new field, but has already shown success. For example,
  reverse engineers a transformer model to understand how it performs
  modular addition. attempts to automate the reverse-engineering
  process, and is somewhat successful. Reverse-engineerings deep neural
  networks (DNN) is a relatively new field, but has already shown
  success. For example, reverse engineers a transformer model to
  understand how it performs modular addition. attempts to automate the
  reverse-engineering process, and is somewhat successful.
  Reverse-engineerings deep neural networks (DNN) is a relatively new
  field, but has already shown success. For example, reverse engineers a
  transformer model to understand how it performs modular addition.
  attempts to automate the reverse-engineering process, and is somewhat
  successful.

  Mechanistic interpretability (MI) posits that deep neural networks
  (DNN) are circuits that can be reverse-engineered to understand their
  inner workings. MI is a relatively new field, but has already shown
  success. For example, reverse engineers a transformer model to
  understand how it performs modular addition. attempts to automate the
  reverse-engineering process, and is somewhat successful.

  Mechanistic interpretability (MI) posits that deep neural networks
  (DNN) are circuits that can be reverse-engineered to understand their
  inner workings. MI is a relatively new field, but has already shown
  success. For example, reverse engineers a transformer model to
  understand how it performs modular addition. attempts to automate the
  reverse-engineering process, and is somewhat successful.

  \section{Background}\label{background}

  \subsection{Transformer models}\label{transformer-models}

  As artificial intelligence systems Mechanistic interpretability (MI)
  posits that deep neural networks (DNN) are circuits that can be
  reverse-engineered to understand their inner workings. MI is a
  relatively new field, but has already shown success. For example,
  \textcite{nanda2023} reverse engineers a transformer model
  \autocite{vaswani2017} to understand how it performs modular addition.
  \textcite{cover2006} attempts to automate the reverse-engineering
  process, and is somewhat successful. However, the process is still
  largely manual and requires a deep understanding of the model's
  architecture and training process. Leveraging the transformer model's
  attention mechanism, \textcite{conmy2023} attempts to automate the
  reverse-engineering process, and is somewhat successful.
  \textcite{conmy2023} attempts to automate the reverse-engineering
  process, and is somewhat successful, while \textcite{belcak2022}
  diconfirms this.

  In this paper, we apply the MI framework to a transformer model
  trained on a dataset of irreducible integers. We show that the model
  has learned to perform modular addition, and we reverse-engineer the
  model to understand how it does so.

  \subsection{Mechanistic
  interpretability}\label{mechanistic-interpretability}

  Mechanistic interpretability (MI) posits that deep neural networks
  (DNN) are circuits that can be reverse-engineered to understand their
  inner workings. MI is a relatively new field, but has already shown
  success. For example, \textcite{nanda2023} reverse engineers a
  transformer model \autocite{vaswani2017} to understand how it performs
  modular addition.

  \subsection{Irreducible integers}\label{irreducible-integers}

  Irreducible integers are primes, though thet are described as such so
  as to allo the title of the paper to be a reference to Douglas
  Hofstades MIU puzzle.

  \section{Methodology}\label{methodology}

  Our methodology consists of the following steps:

  \subsection{Data}\label{data}

  Here's a table:

  \atable{data.csv}{Dataset}{data}

  The dataset consists of four-digit integers and their labels. The
  labels are 1 if the integer is irreducible, and 0 otherwise. The
  dataset is generated by taking all four-digit integers and checking if
  they are irreducible. The dataset is then split into a training set
  and a test set.

  \subsection{Model}\label{model}

  The model is a transformer model in the style of \textcite{hu2023}. It
  is trained on the dataset above.

  \subsection{Reverse-engineering}\label{reverse-engineering}

  We reverse-engineer the model by analyzing the attention weights. We
  show that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition.

  \atable{data.csv}{Dataset}{data}{wide}

  \section{Results}\label{results}

  We reverse-engineer the model by analyzing the attention weights. We
  show that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition. We
  reverse-engineer the model by analyzing the attention weights. We show
  that the model has learned to perform modular addition.

  \subsection{Circuits}\label{circuits}

  We see these circuits:

  \begin{itemize}
  \tightlist
  \item
    Circuit 1
  \item
    Circuit 2
  \item
    Circuit 3
  \end{itemize}

  \subsection{Attention weights}\label{attention-weights}

  We see these attention weights:

  \begin{itemize}
  \tightlist
  \item
    Attention weight 1
  \item
    Attention weight 2
  \end{itemize}

  \subsection{Modular addition}\label{modular-addition}

  We show that the model has learned to perform modular addition. We
  show that the model has learned to perform modular addition. We show
  that the model has learned to perform modular addition. We show that
  the model has learned to perform modular addition. We show that the
  model has learned to perform modular addition. We show that the model
  has learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition. We show that the model has
  learned to perform modular addition.

  \section{Analysis}\label{analysis}

  Lorem Lorem ipsum dolor sit amet, consectetur adipisci elit, sed
  eiusmod tempor incidunt ut labore et dolore magna aliqua. Ut enim ad
  minim veniam, quis nostrum exercitationem ullam corporis suscipit
  laboriosam, nisi ut aliquid ex ea commodi consequatur. Quis aute iure
  reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
  pariatur. Excepteur sint obcaecat cupiditat non proident, sunt in
  culpa qui officia deserunt mollit anim id est laborum.

  \afigure{attn.png}{Attention}{attn}

  \subsection{Interpretability}\label{interpretability}

  Lorem ipsum dolor sit amet, consectetur adipisci elit, sed eiusmod
  tempor incidunt ut labore et dolore magna aliqua. Ut enim ad minim
  veniam, quis nostrum exercitationem ullam corporis suscipit
  laboriosam, nisi ut aliquid ex ea commodi consequatur. Quis aute iure
  reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
  pariatur. Excepteur sint obcaecat cupiditat non proident, sunt in
  culpa qui officia deserunt mollit anim id est laborum.

  \subsection{Generalization}\label{generalization}

  Lorem ipsum dolor sit amet, consectetur adipisci elit, sed eiusmod
  tempor incidunt ut labore et dolore magna aliqua. Ut enim ad minim
  veniam, quis nostrum exercitationem ullam corporis suscipit
  laboriosam, nisi ut aliquid ex ea commodi consequatur. Quis aute iure
  reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
  pariatur. Excepteur sint obcaecat cupiditat non proident, sunt in
  culpa qui officia deserunt mollit anim id est laborum.

  \section{Conclusion}\label{conclusion}

  Lorem ipsum dolor sit amet, consectetur adipisci elit, sed eiusmod
  tempor incidunt ut labore et dolore magna aliqua. Ut enim ad minim
  veniam, quis nostrum exercitationem ullam corporis suscipit
  laboriosam, nisi ut aliquid ex ea commodi consequatur. Quis aute iure
  reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
  pariatur. Excepteur sint obcaecat cupiditat non proident, sunt in
  culpa qui officia deserunt mollit anim id est laborum.
\end{multicols}
\printbibliography % Prints the bibliography
\end{document}