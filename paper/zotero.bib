@article{egri_compositional_2006,
	title = {A Compositional Neural-network Solution to Prime-number Testing},
	abstract = {A long-standing difficulty for connectionism has been to implement compositionality, the idea of building a knowledge representation out of components such that the meaning arises from the meanings of the individual components and how they are combined. Here we show how a neural-learning algorithm, knowledge-based cascade-correlation ({KBCC}), creates a compositional representation of the prime-number concept and uses this representation to decide whether its input n is a prime number or not. {KBCC} conformed to a basic prime-number testing algorithm by recruiting source networks representing division by prime numbers in order from smallest to largest prime divisor up to √n. {KBCC} learned how to test prime numbers faster and generalized better to untrained numbers than did similar knowledge-free neural learners. The results demonstrate that neural networks can learn to perform in a compositional manner and underscore the importance of basing learning on existing knowledge.},
	author = {Egri, László and Shultz, Thomas R},
	date = {2006},
	langid = {english},
	keywords = {/unread},
}

@misc{vaswani2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-06-11},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang2023b,
	title = {Can Transformers Learn to Solve Problems Recursively?},
	url = {http://arxiv.org/abs/2305.14699},
	doi = {10.48550/arXiv.2305.14699},
	abstract = {Neural networks have in recent years shown promise for helping software engineers write programs and even formally verify them. While semantic information plays a crucial part in these processes, it remains unclear to what degree popular neural architectures like transformers are capable of modeling that information. This paper examines the behavior of neural networks learning algorithms relevant to programs and formal verification proofs through the lens of mechanistic interpretability, focusing in particular on structural recursion. Structural recursion is at the heart of tasks on which symbolic tools currently outperform neural models, like inferring semantic relations between datatypes and emulating program behavior. We evaluate the ability of transformer models to learn to emulate the behavior of structurally recursive functions from input-output examples. Our evaluation includes empirical and conceptual analyses of the limitations and capabilities of transformer models in approximating these functions, as well as reconstructions of the ``shortcut" algorithms the model learns. By reconstructing these algorithms, we are able to correctly predict 91 percent of failure cases for one of the approximated functions. Our work provides a new foundation for understanding the behavior of neural networks that fail to solve the very tasks they are trained for.},
	number = {{arXiv}:2305.14699},
	publisher = {{arXiv}},
	author = {Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
	urldate = {2024-04-06},
	date = {2023-06-25},
	eprinttype = {arxiv},
	eprint = {2305.14699 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@online{wu_classification_2023,
	title = {Classification of integers based on residue classes via modern deep learning algorithms},
	url = {https://arxiv.org/abs/2304.01333v3},
	abstract = {Judging whether an integer can be divided by prime numbers such as 2 or 3 may appear trivial to human beings, but can be less straightforward for computers. Here, we tested multiple deep learning architectures and feature engineering approaches on classifying integers based on their residues when divided by small prime numbers. We found that the ability of classification critically depends on the feature space. We also evaluated Automated Machine Learning ({AutoML}) platforms from Amazon, Google and Microsoft, and found that they failed on this task without appropriately engineered features. Furthermore, we introduced a method that utilizes linear regression on Fourier series basis vectors, and demonstrated its effectiveness. Finally, we evaluated Large Language Models ({LLMs}) such as {GPT}-4, {GPT}-J, {LLaMA} and Falcon, and demonstrated their failures. In conclusion, feature engineering remains an important task to improve performance and increase interpretability of machine-learning models, even in the era of {AutoML} and {LLMs}.},
	titleaddon = {{arXiv}.org},
	author = {Wu, Da and Yang, Jingye and Ahsan, Mian Umair and Wang, Kai},
	urldate = {2024-02-27},
	date = {2023-04-03},
	langid = {english},
	doi = {10.1016/j.patter.2023.100860},
}

@misc{humayun2024,
	title = {Deep Networks Always Grok and Here is Why},
	url = {http://arxiv.org/abs/2402.15555},
	doi = {10.48550/arXiv.2402.15555},
	abstract = {Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network ({DNN}) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as {DNNs} initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network ({CNN}) on {CIFAR}10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a {DNN} groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on the local complexity of a {DNN}'s input-output mapping. Our local complexity measures the density of so-called linear regions (aka, spline partition regions) that tile the {DNN} input space and serves as a utile progress measure for training. We provide the first evidence that, for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the {DNN} mapping smoother there) and towards the decision boundary (making the {DNN} mapping less smooth there). Grokking occurs post phase transition as a robust partition of the input space thanks to the linearization of the {DNN} mapping around the training points. Website: https://bit.ly/grok-adversarial},
	number = {{arXiv}:2402.15555},
	author = {Humayun, Ahmed Imtiaz and Balestriero, Randall and Baraniuk, Richard},
	urldate = {2024-06-16},
	date = {2024-06-06},
	eprinttype = {arxiv},
	eprint = {2402.15555 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lee2024,
	title = {Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding},
	url = {http://arxiv.org/abs/2402.03363},
	shorttitle = {Exploring Prime Number Classification},
	abstract = {This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99{\textbackslash}\% in identifying prime numbers and 79{\textbackslash}\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using \$10{\textasciicircum}6\$ integers starting from a specified integer and tested on a different range of \$2 {\textbackslash}times 10{\textasciicircum}6\$ integers extending from \$10{\textasciicircum}6\$ to \$3 {\textbackslash}times 10{\textasciicircum}6\$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of \$3{\textbackslash}times10{\textasciicircum}6\$, we believe that our study contribute to the application of machine learning in prime number analysis. This work aims to demonstrate the potential of such applications and hopes to inspire further exploration and possibilities in diverse fields.},
	number = {{arXiv}:2402.03363},
	publisher = {{arXiv}},
	author = {Lee, Serin and Kim, S.},
	urldate = {2024-02-13},
	date = {2024-02-06},
	eprinttype = {arxiv},
	eprint = {2402.03363 [cs, math]},
	keywords = {11, 68, Computer Science - Machine Learning, G.0, G.1.0, G.1.10, G.1.m, I.0, I.1.1, I.2.0, I.2.6, I.2.m, I.m, J.2, Mathematics - Number Theory},
}

@misc{lin2018,
	title = {Focal Loss for Dense Object Detection},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-{CNN}, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call {RetinaNet}. Our results show that when trained with the focal loss, {RetinaNet} is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	number = {{arXiv}:1708.02002},
	publisher = {{arXiv}},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2024-06-03},
	date = {2018-02-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bronstein2021,
	title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	shorttitle = {Geometric Deep Learning},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as {CNNs}, {RNNs}, {GNNs}, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	number = {{arXiv}:2104.13478},
	publisher = {{arXiv}},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	urldate = {2023-04-25},
	date = {2021-05-02},
	eprinttype = {arxiv},
	eprint = {2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lee2024b,
	title = {Grokfast: Accelerated Grokking by Amplifying Slow Gradients},
	url = {http://arxiv.org/abs/2405.20233},
	shorttitle = {Grokfast},
	abstract = {One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than \${\textbackslash}times 50\$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at https://github.com/ironjr/grokfast.},
	number = {{arXiv}:2405.20233},
	author = {Lee, Jaerin and Kang, Bong Gyun and Kim, Kihoon and Lee, Kyoung Mu},
	urldate = {2024-06-09},
	date = {2024-06-05},
	eprinttype = {arxiv},
	eprint = {2405.20233 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang2024a,
	title = {Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
	url = {http://arxiv.org/abs/2405.15071},
	doi = {10.48550/arXiv.2405.15071},
	shorttitle = {Grokked Transformers are Implicit Reasoners},
	abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, {GPT}-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
	number = {{arXiv}:2405.15071},
	author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
	urldate = {2024-05-28},
	date = {2024-05-26},
	eprinttype = {arxiv},
	eprint = {2405.15071 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ebrahimi_how_2020,
	location = {Online},
	title = {How Can Self-Attention Networks Recognize Dyck-n Languages?},
	url = {https://aclanthology.org/2020.findings-emnlp.384},
	doi = {10.18653/v1/2020.findings-emnlp.384},
	abstract = {We focus on the recognition of Dyck-n (Dn) languages with self-attention ({SA}) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of {SA}, one with a starting symbol ({SA}+) and one without ({SA}-). Our results show that {SA}+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that {SA}- completely breaks down on long sequences whereas the accuracy of {SA}+ is 58.82\%. We find attention maps learned by {SA}+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of {SA} networks is at par with {LSTMs}, which provides evidence on the ability of {SA} to learn hierarchies without recursion.},
	eventtitle = {Findings 2020},
	pages = {4301--4306},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Gelda, Dhruv and Zhang, Wei},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-06-02},
	date = {2020-11},
}

@misc{wang2022a,
	title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 small},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	shorttitle = {Interpretability in the Wild},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how {GPT}-2 small performs a natural language task called indirect object identification ({IOI}). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large {ML} models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	number = {{arXiv}:2211.00593},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	urldate = {2023-06-19},
	date = {2022-11-01},
	eprinttype = {arxiv},
	eprint = {2211.00593 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bhattamishra_ability_2020,
	location = {Online},
	title = {On the Ability and Limitations of Transformers to Recognize Formal Languages},
	url = {https://aclanthology.org/2020.emnlp-main.576},
	doi = {10.18653/v1/2020.emnlp-main.576},
	abstract = {Transformers have supplanted recurrent models in a large number of {NLP} tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that {LSTMs} generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to {LSTMs}, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
	eventtitle = {{EMNLP} 2020},
	pages = {7096--7116},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	urldate = {2024-06-02},
	date = {2020-11},
}

@article{sogaard_opacity_2023,
	title = {On the Opacity of Deep Neural Networks},
	volume = {53},
	rights = {http://creativecommons.org/licenses/by/4.0},
	issn = {0045-5091, 1911-0820},
	url = {https://www.cambridge.org/core/product/identifier/S0045509124000018/type/journal_article},
	doi = {10.1017/can.2024.1},
	abstract = {Deep neural networks are said to be opaque, impeding the development of safe and trustworthy artificial intelligence, but where this opacity stems from is less clear. What are the sufficient properties for neural network opacity? Here, I discuss five common properties of deep neural networks and two different kinds of opacity. Which of these properties are sufficient for what type of opacity? I show how each kind of opacity stems from only one of these five properties, and then discuss to what extent the two kinds of opacity can be mitigated by explainability methods.},
	pages = {224--239},
	number = {3},
	journaltitle = {Canadian Journal of Philosophy},
	shortjournal = {Can. J. of Philosophy},
	author = {Søgaard, Anders},
	urldate = {2024-05-27},
	date = {2023-04},
	langid = {english},
}

@misc{nanda2023,
	title = {Progress measures for grokking via mechanistic interpretability},
	url = {http://arxiv.org/abs/2301.05217},
	abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous {\textbackslash}textit\{progress measures\} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
	number = {{arXiv}:2301.05217},
	publisher = {{arXiv}},
	author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
	urldate = {2023-12-16},
	date = {2023-10-19},
	eprinttype = {arxiv},
	eprint = {2301.05217 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{heil_reproducibility_2021,
	title = {Reproducibility standards for machine learning in the life sciences},
	volume = {18},
	rights = {2021 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01256-7},
	doi = {10.1038/s41592-021-01256-7},
	abstract = {To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.},
	pages = {1132--1135},
	number = {10},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Heil, Benjamin J. and Hoffman, Michael M. and Markowetz, Florian and Lee, Su-In and Greene, Casey S. and Hicks, Stephanie C.},
	urldate = {2024-04-06},
	date = {2021-10},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Data publication and archiving, Machine learning, Standards},
}

@misc{liu2023,
	title = {Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability},
	url = {http://arxiv.org/abs/2305.08746},
	doi = {10.48550/arXiv.2305.08746},
	shorttitle = {Seeing is Believing},
	abstract = {We introduce Brain-Inspired Modular Training ({BIMT}), a method for making neural networks more modular and interpretable. Inspired by brains, {BIMT} embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that {BIMT} discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
	number = {{arXiv}:2305.08746},
	author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
	urldate = {2023-05-18},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2305.08746 [cond-mat, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Representation Theory, Quantitative Biology - Neurons and Cognition},
}

@misc{he2023,
	title = {Simplifying Transformer Blocks},
	url = {http://arxiv.org/abs/2311.01906},
	doi = {10.48550/arXiv.2311.01906},
	abstract = {A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and {MLP} sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and {BERT} encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput, and using 15\% fewer parameters.},
	number = {{arXiv}:2311.01906},
	publisher = {{arXiv}},
	author = {He, Bobby and Hofmann, Thomas},
	urldate = {2024-02-09},
	date = {2023-11-03},
	eprinttype = {arxiv},
	eprint = {2311.01906 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{lipton_mythos_2018,
	title = {The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
	volume = {16},
	issn = {1542-7730},
	url = {https://dl.acm.org/doi/10.1145/3236386.3241340},
	doi = {10.1145/3236386.3241340},
	shorttitle = {The Mythos of Model Interpretability},
	abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
	pages = {31--57},
	number = {3},
	journaltitle = {Queue},
	shortjournal = {Queue},
	author = {Lipton, Zachary C.},
	urldate = {2024-05-27},
	date = {2018-06-01},
}

@article{green_primes_2008,
	title = {The primes contain arbitrarily long arithmetic progressions},
	volume = {167},
	issn = {0003-486X},
	url = {http://annals.math.princeton.edu/2008/167-2/p03},
	doi = {10.4007/annals.2008.167.481},
	abstract = {We prove that there are arbitrarily long arithmetic progressions of primes. There are three major ingredients. The ﬁrst is Szemere´di’s theorem, which asserts that any subset of the integers of positive density contains progressions of arbitrary length. The second, which is the main new ingredient of this paper, is a certain transference principle. This allows us to deduce from Szemere´di’s theorem that any subset of a suﬃciently pseudorandom set (or measure) of positive relative density contains progressions of arbitrary length. The third ingredient is a recent result of Goldston and Yıldırım, which we reproduce here. Using this, one may place (a large fraction of) the primes inside a pseudorandom set of “almost primes” (or more precisely, a pseudorandom measure concentrated on almost primes) with positive relative density.},
	pages = {481--547},
	number = {2},
	journaltitle = {Annals of Mathematics},
	shortjournal = {Ann. Math.},
	author = {Green, Benjamin and Tao, Terence},
	urldate = {2024-02-27},
	date = {2008-03-01},
	langid = {english},
}

@misc{weiss2021,
	title = {Thinking Like Transformers},
	url = {http://arxiv.org/abs/2106.06981},
	doi = {10.48550/arXiv.2106.06981},
	abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language ({RASP}). We show how {RASP} can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a {RASP} solution. In particular, we provide {RASP} programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a {RASP} program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
	number = {{arXiv}:2106.06981},
	publisher = {{arXiv}},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	urldate = {2023-11-23},
	date = {2021-07-19},
	eprinttype = {arxiv},
	eprint = {2106.06981 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{conmy2023,
	title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
	url = {http://arxiv.org/abs/2304.14997},
	doi = {10.48550/arXiv.2304.14997},
	abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the {ACDC} algorithm rediscovered 5/5 of the component types in a circuit in {GPT}-2 Small that computes the Greater-Than operation. {ACDC} selected 68 of the 32,000 edges in {GPT}-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/{ArthurConmy}/Automatic-Circuit-Discovery.},
	number = {{arXiv}:2304.14997},
	publisher = {{arXiv}},
	author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	urldate = {2024-01-21},
	date = {2023-10-28},
	eprinttype = {arxiv},
	eprint = {2304.14997 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{mcleish2024,
	title = {Transformers Can Do Arithmetic with the Right Embeddings},
	url = {http://arxiv.org/abs/2405.17399},
	doi = {10.48550/arXiv.2405.17399},
	abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single {GPU} for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
	number = {{arXiv}:2405.17399},
	author = {{McLeish}, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
	urldate = {2024-05-28},
	date = {2024-05-27},
	eprinttype = {arxiv},
	eprint = {2405.17399 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{hosseini2024,
	title = {You Need to Pay Better Attention},
	url = {http://arxiv.org/abs/2403.01643},
	doi = {10.48550/arXiv.2403.01643},
	abstract = {We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on {MNIST}, {CIFAR}100, {IMDB} Movie Reviews, and Amazon Reviews datasets.},
	number = {{arXiv}:2403.01643},
	author = {Hosseini, Mehran and Hosseini, Peyman},
	urldate = {2024-05-29},
	date = {2024-03-03},
	eprinttype = {arxiv},
	eprint = {2403.01643 [cs]},
	keywords = {68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary), Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.2.6, I.2.7, I.4.0, I.5.0, I.7.0},
}
