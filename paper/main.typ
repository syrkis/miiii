#import "@preview/unequivocal-ams:0.1.1": ams-article, theorem, proof
#import "@preview/equate:0.2.0": equate // <- for numbering equations


#show: equate.with(breakable: true, sub-numbering: true)
#set math.equation(numbering: "(1.1)")

#show: ams-article.with(
  title: [Mechanistic Interpretability and Implementability of Irreducible Integer Identifiers],
  authors: (
    (
      name: "Noah Syrkis",
      // department: [Department of Computer Science],
      // organization: [University of Copenhagen],
      // location: [Copenhagen, Denmark],
      // url: "www.syrkis.com",
    ),
    (
      name: "Anders Søgaard",
      // department: [Department of Computer Science],
      // organization: [University of Copenhagen],
      // location: [Copenhagen, Denmark],
      // url: "www.angbo.com",n
    ),
  ),
  abstract: [
    An attention based deep learning model $cal(M)$ is trained to solve tasks related to prime numbers.
    Specifically, $cal(M)$ is trained to predict if a given natural number $n$ is prime and what, if any,
    prime numbers it can be factorized by. The model is then reverse engineered to understand the
    learned algorithm.
  ],
  bibliography: bibliography("zotero.bib"),
)
// body ///////////////////////////////////////////////////////////////////////

= Introduction

Current state of the art deep learning (DL) models are inherently hard to interpret.
Indeed, DL is subsymbolic in nature, meaning its atomic parts, the individual real numbers of the model weights,
do not in and of themselves convey any meaning to a human observer.
Interpretability is difficult to define @lipton_mythos_2018.

= Related work

*Generalization* — #cite(<power2022>, form: "prose") shows generalization can happen #quote(attribution: cite(<power2022>), "[...] well past the point of overfitting"), dubbing the phenomenon "grokking". This is now well established @nanda2023, @humayun2024, @wang2024a, @conmy2023. By regarding the series of gradients as a stochastic signal, #cite(<lee2024b>, form: "prose") propose decomposing the signal into two components: a fast-varying overfitting component and a slow-varying generalization component. They then show that amplification of the slow varying component significantly accelerates grokking. This is similar to momentum and AdamW, but different, as the authors describe #cite(<lee2024b>, supplement: "p. 8") (I am trying to understand this better).

*Mechanistic interpretability* — #cite(<nanda2023>, form:"prose") trains a transformer model to generalize the modular addition task.
The learned algorithm is then reverse engineered using a qualitative approach (probing, plotting, and guessing).
It is discovered that the generalized circuit uses a discrete Fourier transform (rotation in the complex plane) to solve the problem.
#cite(<conmy2023>, form: "prose") further attempts to automate aspects of the mechanistic interpretability work flow.

*Mechanistic _implementability_* — #cite(<weiss2021>, form: "prose") presents the coding language RASP, which incorporates the architectural constraints of the transformer model into the language itself.
This forces the programmer to be "thinking like a transformer" (which is the title of their paper).
The multi layer perception (MLP) can be thought of as performing a map, performing a function on every element of a set. The attention mechanism can be thought of as a reduce (or map-reduce) operation, where the
attention mechanism is a function that takes a set of elements and

*Primality detection* — Multiple papers describe the use of deep learning to detect prime numbers @egri_compositional_2006, @lee2024, @wu_classification_2023.
None are particularly promising as prime detection algorithms, as they do not procide speedups, use more memory, or are less accurate than traditional methods.
However, in exploring the foundations of deep learning, the task of prime detection is interesting, as it is a simple task that is hard to learn, and is synthetic, meaning that the arbitrary amounts data is generated by a simple algorithm.

*Transformers* — Various modifications/simplifications have been made to the transformer block @he2023, @hosseini2024.
Transformers combine self-attention (a communication mechanism) with feed-forward layers (a computation mechanism).
Importantly, transformers tend to rely on risidual streams (I will elaborate).


= Preliminaries

$cal(M)$ is an attention based deep learning model.
The set of all primes is referred to as $PP$.
A number referred to as $p$ is in $PP$.
A number referred to as $n$ is in $NN$.
The dataset, $cal(D)$, is a set of pairs $[X|Y]$.
$x in X$ and $y in Y$ are elements of the dataset.
$X$ is a matrix of representations of $n in NN < |cal(D)|$.
$Y$ is a one-hot tensor indicating $x in PP$, and which primes $n$ can be factorized by.
There are about $n/ln(n)$ primes less than $n$.
To test if a given number $n$ is prime,
it is sufficient to test if it is divisible by any of the prime numbers less than $sqrt(n)$ (Sieve of Eratosthenes),
of which there are about $sqrt(n)/ln(sqrt(n))$.
The task is refered to as $cal(T)$, with a subscript indicating the task number.


= Methods

Like #cite(<nanda2023>, form: "prose"), $cal(D)$ is constructed from the first 12 769 natural numbers ($113^2$).
The model, $cal(M)$, is trained to predict if a given natural number $n$ is prime ($cal(T)_1$) and what primes it can be factorized by if it is not prime ($cal(T)_2)$. $cal(T)_1$ is strictly harder than $cal(T)_2$, as $cal(T)_1$ is a binary classification indicating failure to factorize by all primes tested for in Task 2. A Task 3, predicting the remainder of the division of $n$ by the prime it is attempted factorized by is also defined, but not used in this paper.

Architectural deceisions are made to align with #cite(<lee2024b>, form: "prose") and #cite(<nanda2023>, form: "prose").
The model is trained using the AdamW optimizer with a learning rate of $10^(-3)$,
and weight decay of $1.0$. Dropout is used with a rate of $0.5$.
a hidden size of 128, a batch size of $|cal(D)|$, and a maximum of 6 000 epochs.
GELU activation is used. Each attention layer has 4 heads (32 dimensions per head).
The MLP layers map the input from 128 to 512 to 128.
Layer normalization is also used.
The gradients were modified in accordance with the method described by #cite(<lee2024b>, form: "prose"),
to speed up generalization.
3 transformer layers are used. The model is trained on a single Apple M3 Metal GPU with JAX.

$cal(T)$ is harder to solve than the modular addition task by #cite(<nanda2023>, form: "prose"),
as it consits of multiple tasks modular muliplication ($n mod p != 0 forall p < sqrt(n/ln(n))$). $T_1$ is being strictly harder than $T_2$,
might merrit and increase in the number of layers, heads, and hidden size,
which I am currently investigating (update for Anders).

#figure(
  image("figs/polar_nats_and_sixes.svg"),
  caption: [
    $n in NN < 2^(10)$, $n in 2NN < 2^(10)$, and $n in 3NN union 6NN < 2^(10)$ in polar coordinates.
  ],
)<nats>

The rotational inclinations of the transformer model shown by #cite(<nanda2023>, form: "prose") motivates the use of a polar coordinate system to visualize the distribution of primes. Each prime number $p$ is mapped to the point $(p, p mod tau)$ in polar coordinates, as seen in @primes and @nats.
One could imagine tigthening and loosening the spiral by multiplying $tau$ by a constant,
to align multiple of a given number in a straight line (imagining this is encouraged).

#figure(
  image("figs/exploration/polar_primes.svg"),
  caption: "The first 2048 primes minus the first 1024 primes.",
)<primes>

The reader is asked to recall that the Sieve of Eratosthenes is an algorithm for finding all prime numbers up to a given limit $n$, by 1) noting that 2 is a prime number, 2) crossing out all multiples of 2, 3) finding the next number that is not crossed out, which is the next prime number, and 4) crossing out all multiples of that number, and so on. Step 2) corresponds to subtracting the center plot in @nats from the left side plot.

The Cheese Cloth of Eratosthenes is a variant of the Sieve of Eratosthenes,
in which the multiples of the prime numbers are not filtered deterministically,
but rather probabilistically—and inappropriately—by using a deep learning model.

// BELOW HERE IS A MESS

This vector was than converted to the desired number system.
// $Y$ was constructed by first querying all prime numbers less than or equal to $n+1$, creating a one hot vector for each sample, in $X$ indicating primality. $Y$ was further augmented by $sqrt(n)$ vectors, each indicating divisibility by the $i$th prime number up to $sqrt(n)$.
Thus, the sum of all $y$ of primes is 1#footnote[With the exception of primes less than $sqrt(n)$ as those are both primes and a multiple of themselves (i.e. 2 is a prime number, but it is also a multiple of 2).], and the sum of all $y$ of non-primes is $>= 1$.

Note that the row sum of $Y$ can be thought of as a sort of measure of how "close" to being prime a given number is. For example, 20 is very much not a prime since it is a multiple of 2, 4, 5 and 10, while 51 (in base 10) looks like a prime but can, in fact, be factorized into 3 and 17.


$Y$ thus includes information about _why_ a given number is not prime.
The inclusion of these extra tasks also allows for interpretability to be on simpler tasks, by training the model on the simpler tasks first, and then training on the more complex tasks.
This allows for comparison of how the model solves the different tasks, when learning them in isolation versus in conjunction.
For each of the $sqrt(n) + 1$ tasks, the focal loss (@focal_loss) and f1 score are calculated every epoch.

$
  L_("focal") = -alpha times (1 - p_t)^gamma times log(p_t)
$
<focal_loss>

The frequency of a positive samples with in task $i$ is used as the weight for the focal loss during training.
Furthermore, a one-hot vector is used to mask tasks, so as to shield the model from a particular signal during training.

= Results

$cal(M)$ was trained on $cal(D)$ for 6 000 epochs, with a batch size of 12 769.
It generalized to some of the $sqrt(n)$ tasks, but not all.
$cal(M)$ thus exists in a paricularaly interesting state,
of partial generalization to the tasks in $cal(T)_2$,
As $cal(T)_1$ is strictly harder than $cal(T)_2$,
the model has not generalized to $cal(T)_1$.
@train_loss_hinton shows the focal loss of the tasks in $cal(T)$ on the train data during training.
@valid_loss_hinton shows the focal loss of the tasks in $cal(T)$ on the validation data during training.

#figure(
  image("figs/latest/train_loss_hinton.svg"),
  caption: [The focal loss of the tasks in $cal(T)$ on the train data during training.],
)<train_loss_hinton>

#figure(
  image("figs/latest/valid_loss_hinton.svg"),
  caption: [The focal loss of the tasks in $cal(T)$ on the validation data during training.],
)<valid_loss_hinton>



= Analysis

= Discussion

= Conclusion

// Bibliography
